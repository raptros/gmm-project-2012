%!TEX root = final_report.tex

\section{Dataset}    

\par Following the previous work in \newcite{wing-baldridge:11} and
\newcite{rolleretal:12}, we use a dataset consisting of Wikipedia articles for
our experiments. Wikipedia is a rich and meaningful source of geographic
information, with a number of geotagged article and natural language text that
talks about those geographic locations, and hence is a good source for
building geographic language models. Besides, the link structure of and
metadata associated with each  article contains additional useful information
that we use for label propagation. 

\par In this project, we use the Wikipedia dump from March 7th, 2012. We use
Textgrounder - the toolkit for preprocessing and geolocating Wikipedia
articles developed for \newcite{wing-baldridge:11}, for our preprocessing
requirements. The Wikipedia dump contains 12,131,628 documents, of which
8,846,243 are actual articles  (as opposed to lists, category pages, talk
pages etc.). 494,740 of these are articles are geotagged - i.e. little more than
5.5\% of the documents in this Wikipedia dump are geotagged articles.  For
label propagation, we restrict ourselves to only those documents that have at
least one incoming our outgoing link to a geotagged document. In other words,
the link graph that we use for label propagation only contains those edges
that have geotagged documents as the source \emph{and} destination vertices. This
leaves us with 273,606 articles for our main experiments.

\par There are several preparation steps required to use this data in our
experiments. The most important, and the most complex, is the generation of
the  link structure graph needed to run label propagation on the corpus. This
preparation involves extracting the internal links (i.e., links to other
articles within Wikipedia) from the Wikipedia articles, assigning each article
a unique node ID, producing a translated link graph, and then extracting the
sub- graph of inter-linked geotagged articles. There are a few open source
projects \footnote{http://haselgrove.id.au/wikipedia.htm}
\footnote{https://github.com/mirkonasato/graphipedia}
\footnote{http://code.google.com/p/wikipedia-netflix/wiki/WikipediaNetflix}
that attempt to create this graph by parsing XML dumps of Wikipedia pages.
However, each of these projects has its idiosyncrasies that makes
interoperability with Textgrounder difficult.  Most importantly, they all use
a different scheme for generating unique article IDs than the one used in
Textgrounder. Reconciling the article IDs between the output of these tools
and Textgrounder is a nontrivial problem. Furthermore, due to different
preprocessing strategies (for dealing with disambiguation pages, Wikipedia
specific pages, external links, unicode support etc.), each of these tools
generates a slightly different set of articles than the one produced by
Textgrounder. In this project we use a slightly modified version of Wikigraph
project \footnote{https://github.com/davidreynolds/wikigraph} to create the
link structure, and perform the matching between the Textgrounder articles and
Wikigraph ourselves.


\par Since we use grid cells as the class labels to be propagated,  another
major preparation step is to label every geographically tagged article  with
the grid cell that its coordinates fall in. We use Textgrounder to generate
the grid for us;  given a set of parameters (grid type, grid cell size etc.),
Textgrounder splits the the world into a number of grid cells, and provides
the information (i.e, bounding coordinates, grid cell centers) for every  non-
empty grid cell that it generates to us. We extract these cells from the
output  log of a Textgrounder run, and then determine the true grid cell for each geo-tagged article.


\par  We give the label propagation system (Junto) a subset of the geolocated
documents with their correct cell assignments as seed (training) labels. We
run the MAD algorithm for label propagation for 10 iterations with the default
parameters. We then evaluate the labellings produced by the algorithm in terms
of precision, recall and mean error distance by comparing the predictions to
the correct cell information.

\par %graph stats, cell stats.  
Our current set of experiments use the 273,606 articles graph; this graph has
2,188,860 edges altogether, and an average degree (number of edges for any
specific node) of 8 with a minimum degree of one and a maximum degree of 21,353. For
comparison, the entire Wikipedia graph has 56,223,737 edges. For our initial
experiments use uniform geographic cells of size 1 degree, yielding 64,800
cells, with 15,982 of them being non-empty (i.e. 24.6\% of the cells contain
at  least 1 document).

\par Grid cell size is a parameter that we have not yet throughly
investigated. There are trade-offs in accuracy related to cell-size -
shrinking cell size increases the number of cells which in turn increases the
number of labels that have to be propagated. This could reduce precision and
recall and increase the run-time for the label propagation algorithm. On the
other hand, \newcite{wing-baldridge:11} report that decreasing cell size
improves results; possibly because it produces more fine-grained cells, which
will reduce the error distances when retrieving incorrect but geographically
close cells. Further work would explore how variation of cell size affects
label propagation results, and additionally explore the use of the kd-trees
adaptive grid of \newcite{rolleretal:12}.
